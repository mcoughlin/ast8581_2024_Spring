{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjHDThjFYHXq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 11 (Wednesday), AST 8581 / PHYS 8581 / CSCI 8581 / STAT 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "With contributions totally ripped off from Siddharth Mishra-Sharma (MIT) and Deep  Chatterjee (MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oxM74nYHXr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where are we headed?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression -> Dimensionality Reduction  -> Principle Component Analysis -> Clustering / Density Estimation / Anomaly Detection -> Supervised Learning -> <b> Deep Learning </b> -> Introduction to Databases - SQL -> Introduction to Databases - NoSQL -> Introduction to Multiprocessing -> Introduction to GPUs -> Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade emcee corner pytorch-lightning tqdm nflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import basinhopping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation-based (likelihood-free) inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Simulation-based inference (SBI) is a powerful class of methods for performing inference in settings where the likelihood is computationally intractable, but simulations can be realized via forward modeling. \n",
    "\n",
    "In this lecture we will\n",
    "- Introduce the notion of an implicit likelihood, and how to leverage it to perform inference;\n",
    "- Look at a \"traditional\" method for likelihood-free inference, Approximate Bayesian Computation (ABC);\n",
    "- Build up two common modern _neural_ SBI techniques: neural likelihood-ratio estimation (NRE) and neural posterior estimation (NPE);\n",
    "- Introduce the concept of statistical coverage testing and calibration.\n",
    "\n",
    "As examples, we will look at a simple Gaussian-signal-on-power-law-background (\"bump hunt\"), where the likelihood is tractable, and a more complicated example of inferring a distribution of point sources, where the likelihood is computationally intractable. We will emphasize what it means for a likelihood to be computationally intractable/challenging and where the advantages of SBI come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/likelihood.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/sbi.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [The frontier of simulation-based inference](https://arxiv.org/abs/1911.01429) (Cranmer, Brehmer, Louppe): Review paper\n",
    "- [simulation-based-inference.org](http://simulation-based-inference.org/): List of papers and resources\n",
    "- [awesome-neural-sbi](https://github.com/smsharma/awesome-neural-sbi): List of papers and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/header.png alt= “” width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple bump-on-power-law example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an initial example, consider a Gaussian signal parameterized by {amplitude, mean location, std} on top of a power law background parameterize by {amplitude, power-law exponent}.\n",
    "$$ x_b = A_b\\,y^{n_b}$$\n",
    "$$x_s = A_s\\,\\exp^{-(y - \\mu_s)^2 / 2\\sigma_s^2}$$\n",
    "$$x \\sim \\mathrm{Pois}(x_b + x_s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b):\n",
    "    \"\"\" Forward model for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n",
    "    \"\"\"\n",
    "    x_b = amp_b * (y ** exp_b)  # Power-law background\n",
    "    x_s = amp_s * np.exp(-((y - mu_s) ** 2) / (2 * std_s ** 2))  # Gaussian signal\n",
    "\n",
    "    x = x_b + x_s  # Total mean signal\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def poisson_interval(k, alpha=0.32): \n",
    "    \"\"\" Uses chi2 to get the poisson interval.\n",
    "    \"\"\"\n",
    "    a = alpha\n",
    "    low, high = (chi2.ppf(a/2, 2*k) / 2, chi2.ppf(1-a/2, 2*k + 2) / 2)\n",
    "    if k == 0: \n",
    "        low = 0.0\n",
    "    return k - low, high - k\n",
    "\n",
    "y = np.linspace(0.1, 1, 50)  # Dependent variable\n",
    "\n",
    "# Mean expected counts\n",
    "x_mu = bump_forward_model(y, \n",
    "                    amp_s=50, mu_s=0.8, std_s=0.05,  # Signal params\n",
    "                    amp_b=50, exp_b=-0.5)  # Background params\n",
    "\n",
    "# Realized counts\n",
    "x = np.random.poisson(x_mu)\n",
    "x_err = np.array([poisson_interval(k) for k in x.T]).T\n",
    "\n",
    "# Plot\n",
    "plt.plot(y, x_mu, color='k', ls='--', label=\"Mean expected counts\")\n",
    "plt.errorbar(y, x, yerr=x_err, fmt='o', color='k', label=\"Realized counts\")\n",
    "\n",
    "plt.xlabel(\"$y$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig(\"assets/x1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The explicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, we can write down a log-likelihood as a Poisson over the mean returned by the forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like(theta, y, x):\n",
    "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s, std_s) on top of a power-law background (amp_b, exp_b).\n",
    "    \"\"\"\n",
    "    amp_s, mu_s, std_s, amp_b, exp_b = theta\n",
    "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    return poisson.logpmf(x, mu).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's focus on just 2 parameters for simplicity, the signal amplitude and mean location. The likelihood in this case is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like_sig(params, y, x):\n",
    "    \"\"\" Log-likehood function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = params\n",
    "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n",
    "    mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    return poisson.logpmf(x, mu).sum()\n",
    "\n",
    "log_like_sig([50, 0.8], y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a maximum-liklelihood estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Initial guess for the parameters\n",
    "initial_guess = [100., 0.1]\n",
    "\n",
    "# Set up the minimizer_kwargs for the basinhopping algorithm\n",
    "minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": ((0, 200), (0, 1))}\n",
    "\n",
    "# Perform the optimization using basinhopping\n",
    "opt = basinhopping(lambda thetas: -log_like_sig(thetas, y, x), initial_guess, minimizer_kwargs=minimizer_kwargs)\n",
    "\n",
    "print(\"MLE parameters: {}; true parameters: {}\".format(opt.x, (50, 0.8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And approximate posterior using `emcee`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(thetas):\n",
    "    \"\"\" Log-prior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = thetas\n",
    "    if 0 < amp_s < 200 and 0 < mu_s < 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "def log_post(thetas, y, x):\n",
    "    \"\"\" Log-posterior function for a Gaussian bump (amp_s, mu_s) on top of a fixed PL background.\n",
    "    \"\"\"\n",
    "    lp = log_prior(thetas)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return lp + log_like_sig(thetas, y, x)\n",
    "    \n",
    "# Sampling with `emcee`\n",
    "ndim, nwalkers = 2, 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(y, x))\n",
    "\n",
    "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler.run_mcmc(pos, 5000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot posterior samples\n",
    "flat_samples = sampler.get_chain(discard=1000, flat=True)\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], smooth=1.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The implicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we will do inference without relying on the explicit likelihood evaluation. The key realization is that samples from the forward model implicitly encode the likelihood; when we are simulating data points $x$ for different parameter points $\\theta$, we are drawing samples from the likelihood:\n",
    "$$x\\sim p(x\\mid\\theta)$$\n",
    "which is where the _implicit_ aspect comes from. Let's write down a bump simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def bump_simulator(thetas, y):\n",
    "    \"\"\" Simulate samples from the bump forward model given theta = (amp_s, mu_s) and abscissa points y.\n",
    "    \"\"\"\n",
    "    amp_s, mu_s = thetas\n",
    "    std_s, amp_b, exp_b = 0.05, 50, -0.5\n",
    "    x_mu = bump_forward_model(y, amp_s, mu_s, std_s, amp_b, exp_b)\n",
    "    x = np.random.poisson(x_mu)\n",
    "    return x\n",
    "\n",
    "# Test it out\n",
    "bump_simulator([50, 0.8], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approximate Bayesian Computation (ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/abc.png alt= “” width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea behind ABC is to realize samples from the forward model (with the parameters $\\theta$ drawn from a prior) and compare it to the dataset of interest $x$. If the data and realized samples are close enough to each other according to some criterion, we keep the parameter points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The comparison criterion here is a simple MSE on the data points. Play around with the parameters of the forward model to see how the criterion `eps` changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_fwd = bump_simulator([50, 0.8], y)\n",
    "eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def abc(y, x, eps_thresh=500., n_samples=1000):\n",
    "    \"\"\"ABC algorithm for Gaussian bump model.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Abscissa points.\n",
    "        x (np.ndarray): Data counts.\n",
    "        eps_thresh (float, optional): Acceptance threshold. Defaults to 500.0.\n",
    "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Accepted samples approximating the posterior p(theta|x).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    total_attempts = 0\n",
    "    progress_bar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n",
    "\n",
    "    # Keep simulating until we have enough accepted samples\n",
    "    while len(samples) < n_samples:\n",
    "        params = np.random.uniform(low=[0, 0], high=[200, 1])  # Priors; theta ~ p(theta)\n",
    "        x_fwd = bump_simulator(params, y)  # x ~ p(x|theta)\n",
    "        eps = np.sum(np.abs(x - x_fwd) ** 2) / len(x)  # Distance metric; d(x, x_fwd)\n",
    "        total_attempts += 1\n",
    "\n",
    "        # If accepted, add to samples\n",
    "        if eps < eps_thresh:\n",
    "            samples.append(params)\n",
    "            progress_bar.update(1)\n",
    "            acceptance_ratio = len(samples) / total_attempts\n",
    "            progress_bar.set_postfix(acceptance_ratio=f\"{acceptance_ratio:.3f}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    return np.array(samples)\n",
    "\n",
    "n_samples = 5_000\n",
    "post_samples = abc(y, x, eps_thresh=200, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(post_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], range=[(0, 200), (0.3, 1)], bins=50);\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[50, 0.8], fig=fig, color=\"red\", weights=np.ones(len(flat_samples)) * n_samples / len(flat_samples), range=[(0, 200), (0.3, 1)], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Downsides of vanilla ABC:\n",
    "- How to summarize the data? Curse of dimensionality / loss of information.\n",
    "- How to compare with data? Likelihood may not be available.\n",
    "- Choice of acceptance threshold: Precision/efficiency tradeoff.\n",
    "- Need to re-run pipeline for new data or new prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural likelihood-ratio estimation (NRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/nre.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For numerical stability, the alternate hypothesis $\\theta_0$ can be assumed to be one where $x$ and $\\theta$ are not correlated, i.e., drawn from the individual marginal distributions $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. Then the alternate has support over the entire parameter space, instead of being a single hypothesis $\\theta_0$.\n",
    "\n",
    "In this case, we get the likelihood-to-evidence ratio,\n",
    "\n",
    "$$\\hat r(x, \\theta) = \\frac{s(x, \\theta)}{1 - s(x, \\theta)} = \\frac{p(x,\\theta)}{p(x)p(\\theta)} = \\frac{p(x\\mid\\theta)}{p(x)}$$\n",
    "\n",
    "$\\hat r(x, \\theta)$ can be shown to be the classifier logit, i.e. the output before softmaxxing into the decision function with range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Start by creating some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_train = 50_000\n",
    "\n",
    "# Simulate training data\n",
    "theta_samples = np.random.uniform(low=[0, 0], high=[200, 1], size=(n_train, 2))  # Parameter proposal\n",
    "x_samples = np.array([bump_simulator(theta, y) for theta in tqdm(theta_samples)])\n",
    "\n",
    "# Convert to torch tensors\n",
    "theta_samples = torch.tensor(theta_samples, dtype=torch.float32)\n",
    "x_samples = torch.tensor(x_samples, dtype=torch.float32)\n",
    "\n",
    "# Normalize the data\n",
    "x_mean = x_samples.mean(dim=0)\n",
    "x_std = x_samples.std(dim=0)\n",
    "x_samples = (x_samples - x_mean) / x_std\n",
    "\n",
    "theta_mean = theta_samples.mean(dim=0)\n",
    "theta_std = theta_samples.std(dim=0)\n",
    "theta_samples = (theta_samples - theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As our parameterized classifier, we will use a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, hidden_dim, output_dim, layers, activation=nn.GELU()):\n",
    "    \"\"\"Create an MLP from the configuration.\"\"\"\n",
    "    seq = [nn.Linear(input_dim, hidden_dim), activation]\n",
    "    for _ in range(layers):\n",
    "        seq += [nn.Linear(hidden_dim, hidden_dim), activation]\n",
    "    seq += [nn.Linear(hidden_dim, output_dim)]\n",
    "    return nn.Sequential(*seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create a neural ratio estimator class, with a corresponding loss function. The loss is a simple binary cross-entropy loss that discriminates between samples from the joint distribution $\\{x, \\theta\\} \\sim p(x\\mid\\theta)$ and those from a product of marginals $\\{x, \\theta\\} \\sim p(x)\\,p(\\theta)$. Samples from the latter are obtained by shuffling joint samples from within a batch.\n",
    "\n",
    "The binary cross-entropy loss is used as the classifier loss to distinguish samples from the joint and marginals,\n",
    "$$\\mathcal L = - \\sum_i y_i \\log(p_i)$$\n",
    "where $y_i$ are the true labels and $p_i$ the softmaxxed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralRatioEstimator(pl.LightningModule):\n",
    "    \"\"\" Simple neural likelihood-to-evidence ratio estimator, using an MLP as a parameterized classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, theta_dim):\n",
    "        super().__init__()\n",
    "        self.classifier = build_mlp(input_dim=x_dim + theta_dim, hidden_dim=128, output_dim=1, layers=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def loss(self, x, theta):\n",
    "\n",
    "        # Repeat x in groups of 2 along batch axis\n",
    "        x = x.repeat_interleave(2, dim=0)\n",
    "\n",
    "        # Get a shuffled version of theta\n",
    "        theta_shuffled = theta[torch.randperm(theta.shape[0])]\n",
    "\n",
    "        # Interleave theta and shuffled theta\n",
    "        theta = torch.stack([theta, theta_shuffled], dim=1).reshape(-1, theta.shape[1])\n",
    "\n",
    "        # Get labels; ones for pairs from joint, zeros for pairs from marginals\n",
    "        labels = torch.ones(x.shape[0], device=x.device) \n",
    "        labels[1::2] = 0.0\n",
    "\n",
    "        # Pass through parameterized classifier to get logits\n",
    "        logits = self(torch.cat([x, theta], dim=1))\n",
    "        probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "        return nn.BCELoss(reduction='none')(probs, labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate loss; initially it should be around -log(0.5) = 0.693\n",
    "nre = NeuralRatioEstimator(x_dim=50, theta_dim=2)\n",
    "nre.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instantiate dataloader and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val_fraction = 0.1\n",
    "batch_size = 128\n",
    "n_samples_val = int(val_fraction * len(x_samples))\n",
    "\n",
    "dataset = TensorDataset(x_samples, theta_samples)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model=nre, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The classifier logits are now an estimator for the likelihood ratio. We can write down a log-likelihood function and use it to sample from the corresponding posterior distribution, just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_like(theta, x):\n",
    "    \"\"\" Log-likelihood ratio estimator using trained classifier logits.\n",
    "    \"\"\"\n",
    "        \n",
    "    x = torch.Tensor(x)\n",
    "    theta = torch.Tensor(theta)\n",
    "\n",
    "    # Normalize\n",
    "    x = (x - x_mean) / x_std\n",
    "    theta = (theta - theta_mean) / theta_std\n",
    "\n",
    "    x = torch.atleast_1d(x)\n",
    "    theta = torch.atleast_1d(theta)\n",
    "\n",
    "    return nre.classifier(torch.cat([x, theta], dim=-1)).squeeze()\n",
    "\n",
    "theta_test = np.array([90, 0.8])\n",
    "x_test = bump_simulator(theta_test, y)\n",
    "\n",
    "log_like(theta_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def log_post(theta, x):\n",
    "    \"\"\" Log-posterior distribution, for sampling.\n",
    "    \"\"\"\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return lp + log_like(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sample with `emcee`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 2, 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(x_test,))\n",
    "\n",
    "pos = opt.x + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler.run_mcmc(pos, 5000, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot approximate posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=1000, flat=True)\n",
    "corner.corner(flat_samples, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural posterior estimation (NPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/npe.png alt= “” width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_flow(d_in=2, d_hidden=32, d_context=16, n_layers=4):\n",
    "    \"\"\" Instantiate a simple (Masked Autoregressive) normalizing flow.\n",
    "    \"\"\"\n",
    "\n",
    "    base_dist = StandardNormal(shape=[d_in])\n",
    "\n",
    "    transforms = []\n",
    "    for _ in range(n_layers):\n",
    "        transforms.append(ReversePermutation(features=d_in))\n",
    "        transforms.append(MaskedAffineAutoregressiveTransform(features=d_in, hidden_features=d_hidden, context_features=d_context))\n",
    "    transform = CompositeTransform(transforms)\n",
    "\n",
    "    flow = Flow(transform, base_dist)\n",
    "    return flow\n",
    "\n",
    "# Instantiate flow\n",
    "flow = get_flow()\n",
    "\n",
    "# Make sure sampling and log-prob calculation makes sense\n",
    "samples, log_prob = flow.sample_and_log_prob(num_samples=100, context=torch.randn(2, 16))\n",
    "print(samples.shape, log_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Construct a neural posterior estimator. It uses a normalizing flow as a (conditional) posterior density estimator, and a feature-extraction network that aligns the directions of variations in parameters $\\theta$ and data $x$.\n",
    "$$  \\mathcal L = -\\log p_\\phi(\\theta\\mid s_\\varphi(x))$$\n",
    "where $\\{\\phi, \\varphi\\}$ are the parameters of the normalizing flow and featurizer MLP, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralPosteriorEstimator(pl.LightningModule):\n",
    "    \"\"\" Simple neural posterior estimator class using a normalizing flow as the posterior density estimator.\n",
    "    \"\"\"\n",
    "    def __init__(self, featurizer, d_context=16):\n",
    "        super().__init__()\n",
    "        self.featurizer = featurizer\n",
    "        self.flow = get_flow(d_in=2, d_hidden=32, d_context=d_context, n_layers=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.featurizer(x)\n",
    "    \n",
    "    def loss(self, x, theta):\n",
    "        context = self(x)\n",
    "        return -self.flow.log_prob(inputs=theta, context=context)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, theta = batch\n",
    "        loss = self.loss(x, theta).mean()\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instantiate the NPE class and look at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = NeuralPosteriorEstimator(featurizer=build_mlp(input_dim=50, hidden_dim=128, output_dim=16, layers=4))\n",
    "npe.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Train using the same data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20)\n",
    "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a test data sample, pass it through the feature extractor, and condition the flow density estimator on it. We get posterior samples by drawing from \n",
    "$$\\theta \\sim p_\\phi(\\theta\\mid s_\\varphi(x)).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta_test = np.array([90, 0.8])\n",
    "x_test = bump_simulator(theta_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "context = npe.featurizer(x_test_norm).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()\n",
    "corner.corner(samples_test, labels=[\"amp_s\", \"mu_s\"], truths=[90, 0.8]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more complicated example: distribution of point sources in a 2D image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let's look at a more complicated example, one that is closer to a typical application of SBI and where the likelihood is formally intractable.\n",
    "\n",
    "The forward model simulates a map of point sources with mean counts drawn from a power law (Pareto) distribution. The distribution of the mean counts is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm dn}{\\mathrm  ds} = A s^{\\beta}\n",
    "$$\n",
    "\n",
    "where $A$ is the amplitude (amp_b), $s$ is the flux, and $\\beta$ is the exponent (exp_b). The fluxes are drawn from a truncated power law with minimum and maximum bounds, $s_\\text{min}$ and $s_\\text{max}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of sources is determined by integrating the power law distribution within the flux limits and taking a Poisson realization:\n",
    "\n",
    "$$\n",
    "N_\\text{sources} \\sim \\text{Pois}\\left(\\int_{s_\\text{min}}^{s_\\text{max}} \\, \\mathrm ds \\frac{\\mathrm dn}{\\mathrm ds}\\right)\n",
    "$$\n",
    "\n",
    "For each source, a position is randomly assigned within the box of size `box_size`. The fluxes are then binned into a grid with `resolution` number of bins in both x and y directions. The resulting map is convolved with a Gaussian point spread function (PSF) with a standard deviation of `sigma_psf` to account for the spatial resolution of the instrument.\n",
    "\n",
    "The output is a 2D map of counts, representing the simulated observation of the point sources in the sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic_2d\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "\n",
    "\n",
    "def simulate_sources(amp_b, exp_b, s_min=0.5, s_max=50.0, box_size=1., resolution=64, sigma_psf=0.01):\n",
    "    \"\"\" Simulate a map of point sources with mean counts drawn from a power law (Pareto) distribution dn/ds = amp_b * s ** exp_b\n",
    "    \"\"\"\n",
    "    # Get number of sources by analytically integrating dn/ds and taking Poisson realization\n",
    "    n_sources = np.random.poisson(-amp_b * (s_min ** (exp_b - 1)) / (exp_b - 1))\n",
    "\n",
    "    # Draw fluxes from truncated power law amp_b * s ** (exp_b - 1), with s_min and s_max as the bounds\n",
    "    fluxes = draw_powerlaw_flux(n_sources, s_min, s_max, exp_b)\n",
    "\n",
    "    positions = np.random.uniform(0., box_size, size=(n_sources, 2))\n",
    "    bins = np.linspace(0, box_size, resolution + 1)\n",
    "\n",
    "    pixel_size = box_size / resolution\n",
    "    kernel = Gaussian2DKernel(x_stddev=1.0 * sigma_psf / pixel_size)\n",
    "\n",
    "    mu_signal = binned_statistic_2d(x=positions[:, 0], y=positions[:, 1], values=fluxes, statistic='sum', bins=bins).statistic\n",
    "    counts = np.random.poisson(convolve(mu_signal, kernel))\n",
    "                \n",
    "    return fluxes, counts\n",
    "\n",
    "def draw_powerlaw_flux(n_sources, s_min, s_max, exp_b):\n",
    "    \"\"\"\n",
    "    Draw from a powerlaw with slope `exp_b` and min/max mean counts `s_min` and `s_max`. From:\n",
    "    https://stackoverflow.com/questions/31114330/python-generating-random-numbers-from-a-power-law-distribution\n",
    "    \"\"\"\n",
    "    u = np.random.uniform(0, 1, size=n_sources)\n",
    "    s_low_u, s_high_u = s_min ** (exp_b + 1), s_max ** (exp_b + 1)\n",
    "    return (s_low_u + (s_high_u - s_low_u) * u) ** (1.0 / (exp_b + 1.0))\n",
    "\n",
    "fluxes, counts = simulate_sources(amp_b=200., exp_b=-1.2)\n",
    "plt.imshow(counts, cmap='viridis', vmax=20)\n",
    "plt.xlabel(\"Pixels\")\n",
    "plt.ylabel(\"Pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw parameters from the prior\n",
    "n_params = 16\n",
    "\n",
    "amp_b_prior = (100., 300.)\n",
    "exp_b_prior = (-2.0, -0.5)\n",
    "\n",
    "amp_bs = np.random.uniform(amp_b_prior[0], amp_b_prior[1], n_params)\n",
    "exp_bs = np.random.uniform(exp_b_prior[0], exp_b_prior[1], n_params)\n",
    "\n",
    "# Plot the data samples on a grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fluxes, counts = simulate_sources(amp_b=amp_bs[i], exp_b=exp_bs[i])\n",
    "    im = ax.imshow(counts, cmap='viridis', vmax=20)\n",
    "    ax.set_title(f'$A_b={amp_bs[i]:.2f}, n_b={exp_bs[i]:.2f}$')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Explicit likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The (marginal) likelihood, which we would need to plug into something like MCMC, is computationally intractable! This is because it involves an integral over a cumbersome latent space, which consists of all possible number $n$ of sources and their positions $\\{z\\}=\\{x, y\\}_{i=1}^{n}$. Let's write this out formally:\n",
    "$$p(x \\mid \\theta)=\\sum_{n} \\int \\mathrm{d}^{n} \\{z\\}\\, p\\left(n \\mid \\theta\\right) \\prod_i^{n} p\\left(z_{i} \\mid \\theta\\right) \\, p\\left(x \\mid \\theta,\\left\\{z_{i}\\right\\}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implicit inference: Neural posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use neural posterior estimation with a normalizing flow again. Get a training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_train = 30_000\n",
    "\n",
    "# Sample from prior, then simulate\n",
    "theta_samples = np.random.uniform(low=[10., -3.], high=[200., -0.99], size=(n_train, 2))\n",
    "x_samples = np.array([simulate_sources(theta[0], theta[1])[1] for theta in tqdm(theta_samples)])\n",
    "\n",
    "# Convert to torch tensors\n",
    "theta_samples = torch.Tensor(theta_samples)\n",
    "x_samples = torch.Tensor(x_samples)\n",
    "\n",
    "# Normalize the data\n",
    "x_mean = x_samples.mean(dim=0)\n",
    "x_std = x_samples.std(dim=0)\n",
    "x_samples = (x_samples - x_mean) / x_std\n",
    "\n",
    "theta_mean = theta_samples.mean(dim=0)\n",
    "theta_std = theta_samples.std(dim=0)\n",
    "theta_samples = (theta_samples - theta_mean) / theta_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "val_fraction = 0.1\n",
    "batch_size = 128\n",
    "n_samples_val = int(val_fraction * len(x_samples))\n",
    "\n",
    "dataset = TensorDataset(x_samples, theta_samples)\n",
    "\n",
    "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val])\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we're working with images, use a simple convolutional neural network (CNN) as the feature extractor. The normalizing flow will be conditioned on the output of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\" Simple CNN feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.unsqueeze(1)  # Add channel dim\n",
    "        \n",
    "        x = self.pool1(F.leaky_relu(self.conv1(x), negative_slope=0.02))\n",
    "        x = self.pool2(F.leaky_relu(self.conv2(x), negative_slope=0.02))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = NeuralPosteriorEstimator(featurizer=CNN(output_dim=32), d_context=32)\n",
    "npe.loss(x_samples[:64], theta_samples[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=15)\n",
    "trainer.fit(model=npe, train_dataloaders=train_loader, val_dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "npe = npe.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get a test map, extract features, condition normalizing flow, extract samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "params_test = np.array([15., -1.4])\n",
    "x_test = simulate_sources(params_test[0], params_test[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "context = npe.featurizer(x_test_norm.unsqueeze(0))\n",
    "\n",
    "samples_test = npe.flow.sample(num_samples=10000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()\n",
    "\n",
    "corner.corner(samples_test, labels=[\"amp\", \"exp\"], truths=params_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test of statistical coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=./assets/coverage.png alt= “” width=800>\n",
    "\n",
    "Figure from 2209.01845."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do some checks to make sure that our posterior has the correct statistical interpretation. In particular, let's test the posterior statistical coverage by evaluating how well the Highest Posterior Density (HPD) intervals capture the true parameter values.\n",
    "\n",
    "The **Highest Posterior Density (HPD)** interval is a region in the parameter space that contains the most probable values for a given credible mass (e.g., 95% or 99%). In other words, it is the shortest interval that contains the specified credible mass of the posterior distribution. This is one of summarizing a posterior distribution.\n",
    "\n",
    "**Nominal coverage** is the probability, or the proportion of the parameter space, that the HPD interval is intended to contain. For example, if the nominal coverage is 0.95, the HPD interval should theoretically contain the true parameter value 95% of the time.\n",
    "\n",
    "**Empirical coverage** is the proportion of true parameter values that actually fall within the HPD interval, based on a set of test cases or simulations.\n",
    "\n",
    "For a perfectly calibrated posterior estimator, empirical coverage = nominal coverage for all credibility levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_test = 200  # How many test samples to draw for coverage test\n",
    "\n",
    "# Get samples \n",
    "x_test = torch.Tensor([simulate_sources(params_test[0], params_test[1])[1] for _ in range(n_test)])\n",
    "x_test_norm = (torch.Tensor(x_test) - x_mean) / x_std\n",
    "\n",
    "# and featurize\n",
    "context = npe.featurizer(x_test_norm)\n",
    "\n",
    "# Get posterior for all samples together in a batch\n",
    "samples_test = npe.flow.sample(num_samples=1000, context=context) * theta_std + theta_mean\n",
    "samples_test = samples_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hpd(samples, credible_mass=0.95):\n",
    "    \"\"\"Compute highest posterior density (HPD) of array for given credible mass.\"\"\"\n",
    "    sorted_samples = np.sort(samples)\n",
    "    interval_idx_inc = int(np.floor(credible_mass * sorted_samples.shape[0]))\n",
    "    n_intervals = sorted_samples.shape[0] - interval_idx_inc\n",
    "    interval_width = np.zeros(n_intervals)\n",
    "    for i in range(n_intervals):\n",
    "        interval_width[i] = sorted_samples[i + interval_idx_inc] - sorted_samples[i]\n",
    "    hdi_min = sorted_samples[np.argmin(interval_width)]\n",
    "    hdi_max = sorted_samples[np.argmin(interval_width) + interval_idx_inc]\n",
    "    return hdi_min, hdi_max\n",
    "\n",
    "hpd(samples_test[0, :, 0], credible_mass=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p_nominals = np.linspace(0.01, 0.99, 50)\n",
    "contains_true = np.zeros((2, n_test, len(p_nominals)))\n",
    "\n",
    "for i_param in range(2):\n",
    "    for i, sample in enumerate(samples_test[:, :, i_param]):\n",
    "        for j, p_nominal in enumerate(p_nominals):\n",
    "            hdi_min, hdi_max = hpd(sample, credible_mass=p_nominal)\n",
    "            if hdi_min < params_test[i_param] < hdi_max:\n",
    "                contains_true[i_param, i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Make two plots, one for each parameter\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(p_nominals, contains_true[0].sum(0) / n_test)\n",
    "ax[0].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "ax[0].set_xlabel(\"Nominal coverage\")\n",
    "ax[0].set_ylabel(\"Expected coverage\")\n",
    "ax[0].set_title(\"Coverage for amplitude\")\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(p_nominals, contains_true[1].sum(0) / n_test)\n",
    "ax[1].plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")\n",
    "ax[1].set_xlabel(\"Nominal coverage\")\n",
    "ax[1].set_ylabel(\"Expected coverage\")\n",
    "ax[1].set_title(\"Coverage for exponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise: Damped Harmonic Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ddot{x}(t) + 2\\beta\\omega_0\\dot{x}(t) + \\omega_0^2x(t) = 0\n",
    "\\end{align}\n",
    "\n",
    "Ansatz, $x = \\exp(\\gamma t)$; $\\gamma = -\\omega_0\\left[\\beta \\pm i\\sqrt{1 - \\beta^2}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damped_sho(t, omega_0, beta, shift=0):\n",
    "    # beta less than 1 for underdamped\n",
    "    envel = beta * omega_0\n",
    "    osc = torch.sqrt(1 - beta**2) * omega_0\n",
    "    tau = t - shift\n",
    "    data = torch.exp(-envel * tau) * torch.cos(osc * tau)\n",
    "    data[tau < 0] = 0  # assume oscillator starts at tau = 0\n",
    "    return data\n",
    "\n",
    "def damped_sho_bilby(t, omega_0, beta, shift=0):\n",
    "    # beta less than 1 for underdamped\n",
    "    envel = beta * omega_0\n",
    "    osc = np.sqrt(1 - beta**2) * omega_0\n",
    "    tau = t - shift\n",
    "    data = np.exp(-envel * tau) * np.cos(osc * tau)\n",
    "    data[tau < 0] = 0  # assume oscillator starts at tau = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_0 = torch.tensor(1.94)\n",
    "beta = torch.tensor(0.4)\n",
    "\n",
    "t_vals = torch.linspace(-1, 10, 200)\n",
    "for shift in [-1, 0, 1, 2]:\n",
    "    x_vals = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    plt.plot(t_vals, x_vals, label=f\"{omega_0=}; {beta=}; {shift=}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Disp\")\n",
    "plt.axvline(x=0, linestyle='dotted', color='black')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters = dict(omega_0=omega_0, beta=beta, shift=2)\n",
    "print(injection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "t_vals = np.linspace(-1, 10, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = damped_sho(t_vals, **injection_parameters) + np.random.normal(0, sigma, t_vals.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_vals, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(t_vals, damped_sho(t_vals, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.prior import Uniform, DeltaFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['omega_0'] = Uniform(0.1, 2, name='omega_0', latex_label='$\\omega_0$') #np array\n",
    "priors['beta'] = Uniform(0, 0.5, name='beta', latex_label='$\\\\beta$') #np array\n",
    "priors['shift'] = Uniform(-4, 4, name='shift', latex_label='$\\Delta\\;t$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_data(omega_0=None, beta=None, shift=None, num_points=1):\n",
    "    \"\"\"Sample omega, beta, shift and return a batch of data with noise\"\"\"\n",
    "    omega_0 = priors['omega_0'].sample() if omega_0 is None else omega_0\n",
    "    omega_0 = torch.tensor(omega_0).to(dtype=torch.float32)\n",
    "    beta = priors['beta'].sample() if beta is None else beta\n",
    "    beta = torch.tensor(beta).to(dtype=torch.float32)\n",
    "    shift = priors['shift'].sample() if shift is None else shift\n",
    "    shift = torch.tensor(shift).to(dtype=torch.float32)\n",
    "\n",
    "    t_vals = torch.linspace(-1, 10, num_points).to(dtype=torch.float32) #\n",
    "\n",
    "    y = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    y += sigma * torch.randn(size=y.size()).to(dtype=torch.float32)\n",
    "\n",
    "    return t_vals, y, omega_0, beta, shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_simulations = 100000\n",
    "num_repeats = 10\n",
    "\n",
    "theta_unshifted_vals = []\n",
    "theta_shifted_vals = []\n",
    "data_unshifted_vals = []\n",
    "data_shifted_vals = []\n",
    "\n",
    "for ii in range(num_simulations):\n",
    "    # generated data with a fixed shift\n",
    "    t_vals, y_unshifted, omega, beta, shift = get_data(num_points=num_points, shift=1)\n",
    "    # create repeats\n",
    "    theta_unshifted = torch.tensor([omega, beta, shift]).repeat(num_repeats, 1).to(device=device)\n",
    "    theta_unshifted_vals.append(theta_unshifted)\n",
    "    data_unshifted_vals.append(y_unshifted.repeat(num_repeats, 1).to(device=device))\n",
    "    # generate shifted data\n",
    "    theta_shifted = []\n",
    "    data_shifted = []\n",
    "    for _ in range(num_repeats):\n",
    "        t_val, y_shifted, _omega, _beta, shift = get_data(\n",
    "            omega_0=omega, beta=beta,  # omega and beta same as above\n",
    "            shift=None,\n",
    "            num_points=num_points\n",
    "        )\n",
    "        theta_shifted.append(torch.tensor([omega, beta, shift]))\n",
    "        data_shifted.append(y_shifted)\n",
    "    theta_shifted_vals.append(torch.stack(theta_shifted).to(device=device))\n",
    "    data_shifted_vals.append(torch.stack(data_shifted).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (\n",
    "            theta_shifted_vals[idx].to(dtype=torch.float32),\n",
    "            theta_unshifted_vals[idx].to(dtype=torch.float32),\n",
    "            data_shifted_vals[idx].to(dtype=torch.float32),\n",
    "            data_unshifted_vals[idx].to(dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, t, d, _ = dataset[6]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (omega, beta, shift), points in zip(t, d):\n",
    "    ax.plot(t_vals.clone().detach().cpu().numpy(), points.clone().detach().cpu().numpy(),\n",
    "            'o', markersize=0.8)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f\"Augmented sample; $\\\\omega_0$ = {t[0][0]:.1f}; $\\\\beta$ = {t[0][1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_data, batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta, _, data_aug, data_orig in train_data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_loader), len(val_data_loader), len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.shape, data_aug.shape, data_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  No Embedding Net used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.distributions import StandardNormal\n",
    "from nflows.flows import Flow\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms import CompositeTransform, RandomPermutation\n",
    "\n",
    "import nflows.utils as torchutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transforms = 5\n",
    "num_blocks = 4\n",
    "hidden_features = 50\n",
    "\n",
    "context_features = num_points\n",
    "\n",
    "base_dist = StandardNormal([2])\n",
    "\n",
    "transforms = []\n",
    "for _ in range(num_transforms):\n",
    "    block = [\n",
    "        MaskedAffineAutoregressiveTransform(\n",
    "            features=2,  # 2-dim posterior\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=context_features,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=torch.tanh,\n",
    "            use_batch_norm=False,\n",
    "            use_residual_blocks=True,\n",
    "            dropout_probability=0.01,\n",
    "        ),\n",
    "        RandomPermutation(features=2)\n",
    "    ]\n",
    "    transforms += block\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of trainable parameters: \", sum(p.numel() for p in flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_augmentations = 10\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(train_data_loader, 1):\n",
    "        augmented_theta, _, augmented_data, _ = val\n",
    "        augmented_theta = augmented_theta[...,0:2]\n",
    "\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            flow_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 10 == 0:\n",
    "            last_loss = running_loss / 10 # avg loss\n",
    "            print(' Avg. train loss/batch after {} batches = {:.4f}'.format(idx, last_loss))\n",
    "            tb_x = epoch_index * len(train_data_loader) + idx\n",
    "            tb_writer.add_scalar('Flow Loss/train', last_loss, tb_x)\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def val_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(val_data_loader, 1):\n",
    "        augmented_theta, _, augmented_data, _ = val\n",
    "        augmented_theta = augmented_theta[...,0:2]\n",
    "\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 5 == 0:\n",
    "            last_loss = running_loss / 5\n",
    "            tb_x = epoch_index * len(val_data_loader) + idx + 1\n",
    "            tb_writer.add_scalar('Flow Loss/val', last_loss, tb_x)\n",
    "\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    tb_writer.flush()\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(flow.parameters(), lr=1e-3, momentum=0.5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"damped-harmonic-oscillator-baseline.neurips\", comment=\"With LR=1e-3\", flush_secs=5)\n",
    "# epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH2 = './damped-harmonic-oscillator-baseline.neurips.pth'\n",
    "flow.load_state_dict(torch.load(PATH2, map_location=device))\n",
    "flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "    # Gradient tracking\n",
    "    flow.train(True)\n",
    "    # flow._embedding_net.train(False)\n",
    "    # no gradient tracking for embedding layer\n",
    "    for name, param in flow._embedding_net.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    avg_train_loss = train_one_epoch(epoch, writer)\n",
    "\n",
    "    # no gradient tracking, for validation\n",
    "    flow.train(False)\n",
    "    avg_val_loss = val_one_epoch(epoch, writer)\n",
    "\n",
    "    print(f\"Train/Val flow Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH2 = './damped-harmonic-oscillator-baseline.notebook.pth'\n",
    "# torch.save(flow.state_dict(), PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys({'omega_0', 'beta'})\n",
    "    injections['omega_0'] = float(truth.numpy()[0])\n",
    "    injections['beta'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys({'omega_0', 'beta'})\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['omega_0'] = samples_numpy.T[0].flatten()\n",
    "    posterior['beta'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "\n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot_samples(samples, truth):\n",
    "    print(truth)\n",
    "    clear_output(wait=True)\n",
    "    sleep(0.5)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.05, 0.5, 0.95],\n",
    "        show_titles=True,\n",
    "        labels=[\"omega_0\", \"beta\",],\n",
    "        truth=truth,\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, (_, theta_test, data_test, data_orig) in enumerate(test_data_loader):\n",
    "    if idx % 1000 !=0: continue \n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0][0].reshape((1, 200)))\n",
    "    live_plot_samples(samples.cpu().reshape(3000,2), theta_test[0][0].cpu()[...,0:2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PP plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, (_, theta_test, data_test, data_unshifted) in enumerate(test_data_loader):\n",
    "    if idx == 1000: break  # full set takes time\n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0][0].reshape((1, 200)))\n",
    "    results.append(\n",
    "        cast_as_bilby_result(samples.cpu().reshape(3000, 2),\n",
    "                             theta_test[0][0].cpu()[...,0:2]))\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    bilby.result.make_pp_plot(results, save=False, keys=['omega_0', 'beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with bilby and analytic result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "bilby_result = bilby.result.Result.from_json('./bilby_sho.json', outdir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilby_result.injection_parameters.pop('shift')\n",
    "injection_parameters = bilby_result.injection_parameters.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_sim_result = bilby.result.Result.from_json(filename='./sim_flow_sho.json', outdir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = damped_sho_bilby(t_vals, **injection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x_vals = torch.from_numpy(x_vals).to(device, dtype=torch.float32)\n",
    "except TypeError:\n",
    "    x_vals = x_vals.to(device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flow_samples = flow.sample(3000, context=x_vals.reshape((1, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_result = cast_as_bilby_result(flow_samples.cpu().reshape(3000, 2),\n",
    "                                   torch.tensor(list(injection_parameters.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = bilby.result.plot_multiple(\n",
    "    [bilby_result, flow_sim_result, flow_result],\n",
    "    labels=['Nested Samp.', 'Flow with Rep. (99K params)', 'Flow Baseline. (355K params)'],\n",
    "    truth=injection_parameters,\n",
    "    quantiles=(0.16, 0.84),\n",
    "    titles=True, save=True,\n",
    "    filename='sho-comparison-bilby-with-sim.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, c):\n",
    "    return m*x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "injection_parameters = dict(m=0.8, c=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "x = np.linspace(-4, 4, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = linear_model(x, **injection_parameters) + np.random.normal(0, sigma, x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(x, linear_model(x, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m_hat = (x * data).sum() / (x * x).sum()\n",
    "c_hat = data.sum() / num_points\n",
    "\n",
    "delta_m = sigma/np.sqrt(np.sum((x)**2))\n",
    "delta_c = sigma * np.sqrt(1/num_points)\n",
    "\n",
    "print(\"Expected m = {:.3f} +/- {:.3f}\".format(m_hat, delta_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Expected c = {:.3f} +/- {:.3f}\".format(c_hat, delta_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.likelihood import GaussianLikelihood\n",
    "from bilby.core.prior import Uniform, DeltaFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['m'] = Uniform(-3, 3, name='m', latex_label='m')\n",
    "priors['c'] = Uniform(-3, 3, name='c', latex_label='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "log_l = GaussianLikelihood(x, data, linear_model, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result = bilby.run_sampler(\n",
    "        likelihood=log_l, priors=priors, sampler='dynesty',\n",
    "        nlive=500, npool=4, save=False, clean=True,\n",
    "        injection_parameters=injection_parameters,\n",
    "        outdir='./linear_regression',\n",
    "        label='linear_regression'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result.plot_corner(priors=True, quantiles=(0.16, 0.84))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Same problem using likelihood-free inference\n",
    "By posterior estimation using a normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can simulate signals\n",
    "  - Given $\\mathbf{\\Theta}_i \\sim p(\\mathbf{\\Theta}) \\rightarrow$ generate signal $f(\\mathbf{\\Theta}_i)$\n",
    "  - Add instrument noise $f(\\mathbf{\\Theta}_i) + n \\rightarrow \\mathbf{d}_i$\n",
    "- We can \"easily\" get pairs $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- From $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\} \\rightarrow p(\\mathbf{\\Theta}, \\mathbf{d}), p(\\mathbf{\\Theta}\\vert \\mathbf{d}), p(\\mathbf{d}\\vert\\mathbf{\\Theta})$\n",
    "- Here, we are interested in the posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The distributions are complex\n",
    "- A solution is to use a Normalizing flow\n",
    "  - Contains a **learnable transform** (a trainable neural network)\n",
    "  - A **base distribution** (often taken to be normal)\n",
    "- Here we use a affine-autogressive transform from `pyro`\n",
    "- A standard normal base distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot_samples(samples, truth):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.16, 0.5, 0.84],\n",
    "        show_titles=True, labels=[\"m\", \"c\"],\n",
    "        truth=truth\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")\n",
    "\n",
    "\n",
    "def live_plot_bilby_result(result, **kwargs):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    result.plot_corner(priors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(m=None, c=None, num_points=1):\n",
    "    \"\"\"Sample m, c and return a batch of data with noise\"\"\"\n",
    "    m = priors['m'].sample() if m is None else m\n",
    "    c = priors['c'].sample() if c is None else c\n",
    "    x = np.linspace(-4, 4, num_points)\n",
    "    y = m*x + c\n",
    "    y += sigma*np.random.normal(size=x.size)\n",
    "\n",
    "    return x, y, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Generate simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lightning import pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 40000\n",
    "theta_vals = []\n",
    "data_vals = []\n",
    "for ii in range(num_simulations):\n",
    "    x_val, y_val, m_val, c_val = get_data(num_points=num_points)\n",
    "    data_vals.append(y_val)\n",
    "    theta_vals.append([m_val, c_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "theta_vals = torch.from_numpy(np.array(theta_vals)).to(torch.float32)\n",
    "data_vals = torch.from_numpy(np.array(data_vals)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return theta_vals[idx], data_vals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 1\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyro.nn import ConditionalAutoRegressiveNN\n",
    "from pyro.distributions import ConditionalTransformedDistribution\n",
    "from pyro.distributions.transforms import ConditionalAffineAutoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MADE in Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick recap of terminology\n",
    "- **Dataset**: the entire dataset, tuples of $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- **Dataloader**: partition of dataset into batches i.e. gives a batch of data\n",
    "- **Training loop**: One epoch\n",
    "  - Push a batch of data\n",
    "  - Calculate loss\n",
    "  - Compute all gradients\n",
    "  - Change all weights based on gradients\n",
    "  - Repeats until all batches are done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pytorch-lightning removes boilerplate code\n",
    "- `torch.nn.Module` $\\rightarrow$ `pl.LightningModule`\n",
    "- Provides methods: `training_step`, `validation_step`, `test_step`, `configure_optimizers`\n",
    "- No need of explicit training loop\n",
    "- Automatic checkpoints, several options for logging\n",
    "- Easily scales to multi-GPU distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys(injection_parameters)\n",
    "    injections['m'] = float(truth.numpy()[0])\n",
    "    injections['c'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys(injection_parameters)\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['m'] = samples_numpy.T[0].flatten()\n",
    "    posterior['c'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "    \n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MADE(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        context_dim,\n",
    "        hidden_dim,\n",
    "        learning_rate: float = LR,\n",
    "        batch_size: int = TRAIN_BATCH_SIZE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n",
    "        self.hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)\n",
    "        self.transform = ConditionalAffineAutoregressive(self.hypernet)\n",
    "        self.flow = ConditionalTransformedDistribution(self.base_dist, [self.transform])\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def log_prob(self, theta, data):\n",
    "        return self.flow.condition(data).log_prob(theta).mean()\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        samples = self.flow.condition(data).sample([2000])\n",
    "        res = cast_as_bilby_result(samples, theta[0])\n",
    "        self.test_results.append(res)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = self.transform.parameters()\n",
    "        optimizer = torch.optim.AdamW(parameters, self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.learning_rate,\n",
    "            pct_start=0.1,\n",
    "            total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        scheduler_config = dict(scheduler=scheduler, interval=\"step\")\n",
    "        return dict(optimizer=optimizer, lr_scheduler=scheduler_config)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PPPlotCallback(pl.Callback):\n",
    "    def on_test_start(self, trainer, pl_module):\n",
    "        pl_module.test_results = []\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            bilby.result.make_pp_plot(pl_module.test_results, save=False, keys=['m', 'c'])\n",
    "        pl_module.test_results.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "context_dim = num_points\n",
    "hidden_dims = [5*input_dim, 5*input_dim]\n",
    "\n",
    "model = MADE(input_dim, context_dim, hidden_dims, batch_size=40, learning_rate=5e-3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    log_every_n_steps=100,\n",
    "    logger=pl.loggers.CSVLogger(\"logs\", name=\"made-expt\"),\n",
    "    callbacks=[PPPlotCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Example posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (theta_test, data_test) in enumerate(test_data):\n",
    "    if idx == 5: break \n",
    "    with torch.no_grad():\n",
    "        samples = model.flow.condition(data_test).sample([1000])\n",
    "    live_plot_samples(samples, theta_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flow_samples = model.flow.condition(\n",
    "        torch.from_numpy(data).unsqueeze(0).to(dtype=torch.float32)\n",
    "    ).sample([1000])\n",
    "truth = np.array(list(injection_parameters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "live_plot_samples(flow_samples, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
